# Evaluating Punctuation Restoration

This includes both restoring punctuation and capitalization. These notes are written for fastpunct, but will apply to other restoration modules if we build them.

### Current situation

We have set up two informal evaluations and associated visualizations.

<u>Edit distance</u>. The edit distance approach takes a gold standard transcript (T1), lower cases it and removes all punctuation (creating T2), and then runs fastpunct (creatingT3).

- T1 → remove capitals and punctuation → T2 → fastpunct → T3

It then mesures the edit distance e12 between T1 and T2 and the edit distance e13 between T1 and T3. With these measure we see e12 as the high edit distance that needs to be reduced by running fastpunct. And e13 ideally is 0, that is, the original is restored. Here is a typical result:

```
>>> 22 14

=== Intro Good evening. Leading the news this Thursday, three private contributors of contra money told their stories to the Iran contra hearing. A Navy Board of Inquiry began its investigation of the U. S. S. Stark tragedy and President Reagan

=== intro good evening leading the news this thursday three private contributors of contra money told their stories to the iran contra hearing a navy board of inquiry began its investigation of the u s s stark tragedy and president reagan

=== Int., Good evening leading the news this Thursday, three private contributors of Contra Money told their stories to the Iran Contra hearing, a Navy Board of Inquiry began its investigation of the U. S.'s stark tragedy and President Reagan.
```

Here, e12 is 22 and e13 is 14, showing that the module restores some of the gold standard punctuation and capitalization. Sometimes the edit distance goes up significantly after running fastpunct (by an order of magnitude in some examples). This indicates cases where fastpunct duplicates text from the input in the output.

<u>Effectiveness of NER</u>. With this approach we look at the differences in the outcome of two pipelines on transcript data automatically generated by Kaldi:

1. Kaldi → spaCy NER
2. Kaldi → fastpunct → spacy NER

We then run a visualizer that shows how many entities were found by both pipelines.

<table>
<tr>
<td><img src="kaldi-spacy.png" width="400"/></td>
<td><img src="kaldi-fastpunct-spacy.png" width="400"/></td>
</tr>
</table>
This particualr case shows that without fastpunct we find only about 25% of entities that were found with the full pipeline.

### Evaluation plan

We aim for three kinds of evaluation:

1. We will use edit distance as above, but will spend some time building a small set of transcripts (initially probably just a dozen or so) and transcript fragments (maybe about a hundred) that we run this over.
2. We also want to use edit distance on real data with transcripts created with Kaldi and updated transcripts from FixIt. Here we have three kinds of data: output of Kaldi (T1), output of running fastpunct on T1 (T2) and the output of the FixIt process on T1 (T3). We compare the edit distance from T1-T2 (e12) to the edit distance of T2-T3 (e23), where e23 should be trending to 0. This assumes a set of unedited and edited transcripts, at the moment we have 17 examples of this already.
3. We will also use the comparison on how effective NER is given absence or presence of fastpunct. But we do not only report on how many entities were found, we also report precision and recall for the results of both pipelines. For this we take a few transcripts or transcript segments after Kaldi and after Kaldi+fastpunct and add entity annotation for both. We will not include entity linking. This amounts to some extra entity annotation on top of the annotation done for the NER component.

